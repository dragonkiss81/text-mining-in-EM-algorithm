x[i] <- rnorm(1, mean=1)
y[i] <- "heads"
} else {
x[i] <- rnorm(1, mean=3)
y[i] <- "tails"
}
}
densityplot( ~x,
par.settings = list(
plot.symbol = list(
col=as.factor(y)
)
)
)
## set the initial guesses for the distribution parameters
mu_1 <- 0
mu_2 <- 1
## as well as the latent variable parameters
tau_1 <- 0.5
tau_2 <- 0.5
for( i in 1:1000 ) {
## Given the observed data, as well as the distribution parameters,
## what are the latent variables?
T_1 <- tau_1 * dnorm( x, mu_1 )
T_2 <- tau_2 * dnorm( x, mu_2 )
P_1 <- T_1 / (T_1 + T_2)
P_2 <- T_2 / (T_1 + T_2) ## note: P_2 = 1 - P_1
tau_1 <- mean(P_1)
tau_2 <- mean(P_2)
## Given the observed data, as well as the latent variables,
## what are the population parameters?
mu_1 <- sum( P_1 * x ) / sum(P_1)
mu_2 <- sum( P_2 * x ) / sum(P_2)
## print the current estimates
print( c(mu_1, mu_2, mean(P_1)) )
}
library("ggplot2")
library("lattice")
# There's three separate elements:
#
# The observed data,
# The distribution parameters, (Normal distribution means)
# The latent variables. (the 'mixture' probabilities)
# The EM algorithm bounces back and forth between two processes:
#
# 1. Given the current parameters and the observed data, estimate the latent variables.
# 2. Given the observed data and the latent variables, estimate the parameters.
# Suppose we flip a biased coin that comes up heads 25% \( (p=0.25) \) of the time.
# If it's heads, we draw from a Normal distribution with mean 1, standard deviation 1.
# If it's tails, we draw from a Normal distribution with mean 7, standard deviation 1.
# We repeat this process 1000 times.
set.seed(123)
tau_1_true <- 0.30
x <- y <- vector(mode="numeric", length=1000)
for( i in 1:1000 ) {
if( runif(1) < tau_1_true ) {
x[i] <- rnorm(1, mean=1)
y[i] <- "heads"
} else {
x[i] <- rnorm(1, mean=3)
y[i] <- "tails"
}
}
densityplot( ~x,
par.settings = list(
plot.symbol = list(
col=as.factor(y)
)
)
)
## set the initial guesses for the distribution parameters
mu_1 <- 0
mu_2 <- 1
## as well as the latent variable parameters
tau_1 <- 0.5
tau_2 <- 0.5
for( i in 1:1000 ) {
## Given the observed data, as well as the distribution parameters,
## what are the latent variables?
T_1 <- tau_1 * dnorm( x, mu_1 )
T_2 <- tau_2 * dnorm( x, mu_2 )
P_1 <- T_1 / (T_1 + T_2)
P_2 <- T_2 / (T_1 + T_2) ## note: P_2 = 1 - P_1
tau_1 <- mean(P_1)
tau_2 <- mean(P_2)
## Given the observed data, as well as the latent variables,
## what are the population parameters?
mu_1 <- sum( P_1 * x ) / sum(P_1)
mu_2 <- sum( P_2 * x ) / sum(P_2)
## print the current estimates
print( c(mu_1, mu_2, mean(P_1)) )
}
library("ggplot2")
library("lattice")
# There's three separate elements:
#
# The observed data,
# The distribution parameters, (Normal distribution means)
# The latent variables. (the 'mixture' probabilities)
# The EM algorithm bounces back and forth between two processes:
#
# 1. Given the current parameters and the observed data, estimate the latent variables.
# 2. Given the observed data and the latent variables, estimate the parameters.
# Suppose we flip a biased coin that comes up heads 25% \( (p=0.25) \) of the time.
# If it's heads, we draw from a Normal distribution with mean 1, standard deviation 1.
# If it's tails, we draw from a Normal distribution with mean 7, standard deviation 1.
# We repeat this process 1000 times.
set.seed(122)
tau_1_true <- 0.30
x <- y <- vector(mode="numeric", length=1000)
for( i in 1:1000 ) {
if( runif(1) < tau_1_true ) {
x[i] <- rnorm(1, mean=1)
y[i] <- "heads"
} else {
x[i] <- rnorm(1, mean=3)
y[i] <- "tails"
}
}
densityplot( ~x,
par.settings = list(
plot.symbol = list(
col=as.factor(y)
)
)
)
## set the initial guesses for the distribution parameters
mu_1 <- 0
mu_2 <- 1
## as well as the latent variable parameters
tau_1 <- 0.5
tau_2 <- 0.5
for( i in 1:1000 ) {
## Given the observed data, as well as the distribution parameters,
## what are the latent variables?
T_1 <- tau_1 * dnorm( x, mu_1 )
T_2 <- tau_2 * dnorm( x, mu_2 )
P_1 <- T_1 / (T_1 + T_2)
P_2 <- T_2 / (T_1 + T_2) ## note: P_2 = 1 - P_1
tau_1 <- mean(P_1)
tau_2 <- mean(P_2)
## Given the observed data, as well as the latent variables,
## what are the population parameters?
mu_1 <- sum( P_1 * x ) / sum(P_1)
mu_2 <- sum( P_2 * x ) / sum(P_2)
## print the current estimates
print( c(mu_1, mu_2, mean(P_1)) )
}
sum(P_1)
fileNames <- choose.files()
choose.files()
fileNames <- choose.files()
setwd("~/Desktop/EMandBayse/20news/Train/alt.atheism")
fileNames <- choose.files()
fileNames <- choose.file()
setwd("~/Desktop/EMandBayse/20news/Train/alt.atheism")
X <- read.csv(tk_choose.files(caption = "Choose X"))
library(tcltk)
X <- read.csv(tk_choose.files(caption = "Choose X"))
X <- read.csv(tk_choose.files(caption = "Choose X"))
X
X <- read.csv(tk_choose.files(caption = "Choose X"))
XYZ.list <- lapply(tk_choose.files(caption = "Choose X, Y, and Z"), read.csv)
choose.files(default = "", caption = "Select files",
multi = TRUE, filters = Filters,
index = nrow(Filters))
X <- read.csv(tk_choose.files(caption = "Choose X"))
setwd("~/Desktop/EMandBayse/20news/Train/alt.atheism")
X <- read.csv(tk_choose.files(caption = "Choose X"))
XYZ.list <- lapply(tk_choose.files(caption = "Choose X, Y, and Z"), read.csv)
XYZ.list
XYZ.list <- lapply(tk_choose.files(caption = "Choose X, Y, and Z"), read.csv)
XYZ.list <- lapply(tk_choose.files(caption = "Choose X, Y, and Z"), read.csv)
XYZ.list <- lapply(tk_choose.files(caption = "Choose X, Y, and Z"), read.csv)
File.names<-(tk_choose.files(default="", caption="Choose your files", multi=TRUE, filters=NULL, index=1))
File.names
file<- read.table(File.names,header=TRUE)
Num.Files<-NROW(File.names)
file<- read.table(File.names,header=TRUE)
Num.Files
XYZ.list <- lapply(tk_choose.files(caption = "Choose your files"), read.csv)
File.names<-(tk_choose.files(default="", caption="Choose your files", multi=TRUE, filters=NULL, index=1))
filelist = list.files(pattern = ".*")
filelist
datalist = lapply(filelist, function(x)read.table(x, header=T))
datalist = lapply(filelist, FUN=read.table, header=TRUE)
filelist
datalist
lapply(filelist, FUN=read.table, header=TRUE)
filelist = list.files(pattern = ".*")
#assuming tab separated values with a header
datalist = lapply(filelist, FUN=read.table, header=TRUE)
datalist = lapply(filelist, FUN=read.table)
datalist = lapply(filelist, function(x) read.table(x))
read.table(filelist[1])
filelist[1]
read.table("51124")
read.table("51124.txt")
read.table("51124.txt")
temp <- read.table("51124.txt")
temp <- readline("51124.txt")
temp
temp <- readline("51124.txt")
temp
temp <- scan("51124.txt")
temp <- scan("51124.txt", what="character")
wordFreq <- sort(table(tolower(data)))
temp <- scan("51124.txt", what="character")
temp
readLines("53623")
filelist = list.files(pattern = ".*")
datalist = lapply(filelist, function(x) scan(x, what="character"))
datalist
data <- gsub("[^\\w-]", "", datalist, perl=TRUE)
data <- gsub("[^\\w-]", "", datalist, perl=TRUE)
filelist = list.files(pattern = ".*")
datalist = lapply(filelist, function(x) scan(x, what="character"))
wordFreq <- sort(table(tolower(data)))
wordFreq
datalist
library(Snowball)
library(SnowballC)
library(SnowballC)
SnowballStemmer(c('functions', 'stemming', 'liked', 'doing'))
SnowballCStemmer(c('functions', 'stemming', 'liked', 'doing'))
wordStem(c('functions', 'stemming', 'liked', 'doing'))
datalist = lapply(filelist, function(x) wordStem(x))
filelist = list.files(pattern = ".*")
datalist = lapply(filelist, function(x) scan(x, what="character"))
txt <- system.file("texts", "txt", package = "tm")
txt
ovid <- Corpus(DirSource(txt), readerControl = list(language = "lat")))
ovid <- Corpus(DirSource(txt), readerControl = list(language = "lat"))
library(tm)
ovid <- Corpus(DirSource(txt), readerControl = list(language = "lat"))
ovid
setwd("~/Desktop/EMandBayse/20news/Train")
txt
txt <- "alt.atheism"
ovid <- Corpus(DirSource(txt), readerControl = list(language = "lat"))
ovid
inspect(ovid[1:2])
ovid[[1]]
ovid[1]
ovid[[2]]
inspect(ovid[[2]])
show(ovid[[2]])
txt <- "alt.atheism"
setwd("~/Desktop/EMandBayse/20news")
txt <- "Test/alt.atheism"
setwd("~/Desktop/EMandBayse/20news")
txt <- "Train/alt.atheism"
my.corpus <- Corpus(DirSource(txt), readerControl = list(language = "lat"))
my.corpus <- tm_map(my.corpus, removeWords, stopwords("english"))
library(SnowballC)
library(tm)
# filelist = list.files(pattern = ".*")
# datalist = lapply(filelist, function(x) scan(x, what="character"))
txt <- "Train/alt.atheism"
my.corpus <- Corpus(DirSource(txt), readerControl = list(language = "lat"))
my.corpus <- tm_map(my.corpus, stripWhitespace)
my.corpus <- tm_map(my.corpus, tolower)
my.corpus <- tm_map(my.corpus, removeWords, stopwords("english"))
dtm <- DocumentTermMatrix(my.corpus)
my.corpus
dtm <- DocumentTermMatrix(my.corpus[[1]])
dtm <- DocumentTermMatrix(my.corpus)
my.corpus
my.corpus[[1]]
dtm <- DocumentTermMatrix(reuters)
dtm <- DocumentTermMatrix(my.corpus)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(my.corpus, content_transformer(tolower))
my.corpus <- Corpus(DirSource(txt), readerControl = list(language = "lat"))
my.corpus <- tm_map(my.corpus, stripWhitespace)
my.corpus <- tm_map(my.corpus, content_transformer(tolower))
my.corpus <- tm_map(my.corpus, content_transformer(stripWhitespace))
my.corpus <- tm_map(my.corpus, content_transformer(tolower))
my.corpus <- tm_map(my.corpus, removeWords, stopwords("english"))
dtm <- DocumentTermMatrix(my.corpus)
dtm <- DocumentTermMatrix(reuters)
dtm <- DocumentTermMatrix(my.corpus)
dtm
dtm
inspect(tdm[1:10, 1:2])
dtm <- TermDocumentMatrix(my.corpus)
inspect(tdm[1:10, 1:2])
tdm <- TermDocumentMatrix(my.corpus)
inspect(tdm[1:10, 1:2])
library(SnowballC)
library(tm)
# filelist = list.files(pattern = ".*")
# datalist = lapply(filelist, function(x) scan(x, what="character"))
txt <- "Train/alt.atheism"
my.corpus <- Corpus(DirSource(txt), readerControl = list(language = "lat"))
my.corpus <- tm_map(my.corpus, stripWhitespace)
my.corpus <- tm_map(my.corpus, removePunctuation)
my.corpus <- tm_map(my.corpus, removeNumbers)
my.corpus <- tm_map(my.corpus, content_transformer(tolower))
my.corpus <- tm_map(my.corpus, removeWords, stopwords("english"))
tdm <- TermDocumentMatrix(my.corpus)
inspect(tdm[1:10, 1:2])
my.corpus <- tm_map(my.corpus, stemDocument)
library(SnowballC)
library(tm)
# filelist = list.files(pattern = ".*")
# datalist = lapply(filelist, function(x) scan(x, what="character"))
txt <- "Train/alt.atheism"
my.corpus <- Corpus(DirSource(txt), readerControl = list(language = "lat"))
my.corpus <- tm_map(my.corpus, stripWhitespace)
my.corpus <- tm_map(my.corpus, content_transformer(stemDocument))
my.corpus <- tm_map(my.corpus, removePunctuation)
my.corpus <- tm_map(my.corpus, removeNumbers)
my.corpus <- tm_map(my.corpus, content_transformer(tolower))
my.corpus <- tm_map(my.corpus, removeWords, stopwords("english"))
tdm <- TermDocumentMatrix(my.corpus)
library(SnowballC)
library(tm)
# filelist = list.files(pattern = ".*")
# datalist = lapply(filelist, function(x) scan(x, what="character"))
txt <- "Train/alt.atheism"
my.corpus <- Corpus(DirSource(txt), readerControl = list(language = "lat"))
my.corpus <- tm_map(my.corpus, stripWhitespace)
my.corpus <- tm_map(my.corpus, removePunctuation)
my.corpus <- tm_map(my.corpus, removeNumbers)
my.corpus <- tm_map(my.corpus, content_transformer(tolower))
# stemming
corpus.copy <- corpus
corpus.temp <- tm_map(my.corpus, stemDocument, language = "english")
my.corpus <- tm_map(corpus.temp, stemCompletion, dictionary = corpus.copy)
my.corpus <- tm_map(my.corpus, removeWords, stopwords("english"))
tdm <- TermDocumentMatrix(my.corpus)
corpus.temp
(a <- c("mining", "miners", "mining"))
(b <- stemDocument(a))
(d <- stemCompletion(b, dictionary=a))
corpus.temp <- tm_map(my.corpus, stemDocument, language = "english")
wordStem2 <- function(x) {
mywords <- unlist(strsplit(x, " "))
mycleanwords <- gsub("^\\W+|\\W+$", "", mywords, perl=T)
mycleanwords <- mycleanwords[mycleanwords != ""]
wordStem(mycleanwords)
}
library(SnowballC)
library(tm)
# filelist = list.files(pattern = ".*")
# datalist = lapply(filelist, function(x) scan(x, what="character"))
wordStem2 <- function(x) {
mywords <- unlist(strsplit(x, " "))
mycleanwords <- gsub("^\\W+|\\W+$", "", mywords, perl=T)
mycleanwords <- mycleanwords[mycleanwords != ""]
wordStem(mycleanwords)
}
txt <- "Train/alt.atheism"
my.corpus <- Corpus(DirSource(txt), readerControl = list(language = "lat"))
my.corpus <- tm_map(my.corpus, stripWhitespace)
my.corpus <- tm_map(my.corpus, removePunctuation)
my.corpus <- tm_map(my.corpus, removeNumbers)
my.corpus <- tm_map(my.corpus, content_transformer(tolower))
my.corpus <- tm_map(my.corpus, wordStem2)
tdm <- TermDocumentMatrix(my.corpus)
library(SnowballC)
library(tm)
# filelist = list.files(pattern = ".*")
# datalist = lapply(filelist, function(x) scan(x, what="character"))
wordStem2 <- function(x) {
mywords <- unlist(strsplit(x, " "))
mycleanwords <- gsub("^\\W+|\\W+$", "", mywords, perl=T)
mycleanwords <- mycleanwords[mycleanwords != ""]
wordStem(mycleanwords)
}
txt <- "Train/alt.atheism"
my.corpus <- Corpus(DirSource(txt), readerControl = list(language = "lat"))
my.corpus <- tm_map(my.corpus, stripWhitespace)
my.corpus <- tm_map(my.corpus, removePunctuation)
my.corpus <- tm_map(my.corpus, removeNumbers)
my.corpus <- tm_map(my.corpus, content_transformer(tolower))
# my.corpus <- tm_map(my.corpus, wordStem2)
my.corpus <- tm_map(my.corpus, removeWords, stopwords("english"))
tdm <- TermDocumentMatrix(my.corpus)
inspect(tdm[1:10, 1:2])
texts <- c("i am member of the XYZ association",
"apply for our open associate position",
"xyz memorial lecture takes place on wednesday",
"vote for the most popular lecturer")
# Step 1: Create corpus
corpus <- Corpus(DataframeSource(data.frame(texts)))
# Step 2: Keep a copy of corpus to use later as a dictionary for stem completion
corpus.copy <- corpus
# Step 3: Stem words in the corpus
corpus.temp <- tm_map(corpus, stemDocument, language = "english")
inspect(corpus.temp)
# Step 4: Complete the stems to their original form
corpus.final <- tm_map(corpus.temp, stemCompletion, dictionary = corpus.copy)
inspect(corpus.final)
txt <- "Train/alt.atheism"
my.corpus <- Corpus(DirSource(txt), readerControl = list(language = "lat"))
my.corpus <- tm_map(my.corpus, stripWhitespace)
my.corpus <- tm_map(my.corpus, removePunctuation)
my.corpus <- tm_map(my.corpus, removeNumbers)
my.corpus <- tm_map(my.corpus, content_transformer(tolower))
# my.corpus <- tm_map(my.corpus, wordStem2)
corpus.copy <- my.corpus
corpus.temp <- tm_map(my.corpus, stemDocument, language = "english")
corpus.final <- tm_map(corpus.temp, stemCompletion, dictionary = corpus.copy)
inspect(corpus.final)
corpus.temp
texts <- c("i am member of the XYZ association",
"apply for our open associate position",
"xyz memorial lecture takes place on wednesday",
"vote for the most popular lecturer")
corpus <- Corpus(DataframeSource(data.frame(texts)))
# Step 2: Keep a copy of corpus to use later as a dictionary for stem completion
corpus.copy <- my.corpus
corpus.temp <- tm_map(my.corpus, stemDocument, language = "english")
corpus.final <- tm_map(corpus.temp, stemCompletion, dictionary = corpus.copy)
inspect(corpus.final)
texts <- c("i am member of the XYZ association",
"apply for our open associate position",
"xyz memorial lecture takes place on wednesday",
"vote for the most popular lecturer")
my.corpus <- Corpus(DataframeSource(data.frame(texts)))
# Step 2: Keep a copy of corpus to use later as a dictionary for stem completion
corpus.copy <- my.corpus
corpus.temp <- tm_map(my.corpus, stemDocument, language = "english")
corpus.final <- tm_map(corpus.temp, stemCompletion, dictionary = corpus.copy)
inspect(corpus.final)
library(SnowballC)
library(tm)
# filelist = list.files(pattern = ".*")
# datalist = lapply(filelist, function(x) scan(x, what="character"))
wordStem2 <- function(x) {
mywords <- unlist(strsplit(x, " "))
mycleanwords <- gsub("^\\W+|\\W+$", "", mywords, perl=T)
mycleanwords <- mycleanwords[mycleanwords != ""]
wordStem(mycleanwords)
}
txt <- "Train/alt.atheism"
my.corpus <- Corpus(DirSource(txt), readerControl = list(language = "lat"))
my.corpus <- tm_map(my.corpus, stripWhitespace)
my.corpus <- tm_map(my.corpus, removePunctuation)
my.corpus <- tm_map(my.corpus, removeNumbers)
my.corpus <- tm_map(my.corpus, content_transformer(tolower))
# my.corpus <- tm_map(my.corpus, wordStem2)
corpus.copy <- my.corpus
corpus.temp <- tm_map(my.corpus, stemDocument, language = "english")
corpus.temp
corpus.temp[[1]]
inspect(corpus.temp[[1]])
(corpus.temp[[1]])
my.corpus <- corpus.temp
my.corpus <- tm_map(my.corpus, removeWords, stopwords("english"))
tdm <- TermDocumentMatrix(my.corpus)
inspect(tdm[1:10, 1:2])
txt <- "Train/alt.atheism"
my.corpus <- Corpus(DirSource(txt), readerControl = list(language = "lat"))
my.corpus <- tm_map(my.corpus, stripWhitespace)
my.corpus <- tm_map(my.corpus, removePunctuation)
my.corpus <- tm_map(my.corpus, removeNumbers)
my.corpus <- tm_map(my.corpus, content_transformer(tolower))
my.corpus <- tm_map(my.corpus, stemDocument, language = "english")
inspect(corpus.final)
my.corpus <- tm_map(my.corpus, removeWords, stopwords("english"))
tdm <- TermDocumentMatrix(my.corpus)
inspect(tdm[1:10, 1:2])
inspect(tdm[1:100, 1:3])
sapply(tdm, sum)
apply(tdm,1, sum)
library(SnowballC)
library(tm)
# filelist = list.files(pattern = ".*")
# datalist = lapply(filelist, function(x) scan(x, what="character"))
wordStem2 <- function(x) {
mywords <- unlist(strsplit(x, " "))
mycleanwords <- gsub("^\\W+|\\W+$", "", mywords, perl=T)
mycleanwords <- mycleanwords[mycleanwords != ""]
wordStem(mycleanwords)
}
txt <- "Train/alt.atheism"
my.corpus <- Corpus(DirSource(txt), readerControl = list(language = "lat"))
my.corpus <- tm_map(my.corpus, stripWhitespace)
my.corpus <- tm_map(my.corpus, removePunctuation)
my.corpus <- tm_map(my.corpus, removeNumbers)
my.corpus <- tm_map(my.corpus, content_transformer(tolower))
# my.corpus <- tm_map(my.corpus, stemDocument, language = "english")
my.corpus <- tm_map(my.corpus, removeWords, stopwords("english"))
tdm <- TermDocumentMatrix(my.corpus)
inspect(tdm[1:100, 1:3])
apply(tdm,1,sum)
c(apply(tdm,1,sum))
library(SnowballC)
library(tm)
# filelist = list.files(pattern = ".*")
# datalist = lapply(filelist, function(x) scan(x, what="character"))
wordStem2 <- function(x) {
mywords <- unlist(strsplit(x, " "))
mycleanwords <- gsub("^\\W+|\\W+$", "", mywords, perl=T)
mycleanwords <- mycleanwords[mycleanwords != ""]
wordStem(mycleanwords)
}
txt <- "Train/alt.atheism"
my.corpus <- Corpus(DirSource(txt), readerControl = list(language = "lat"))
my.corpus <- tm_map(my.corpus, stripWhitespace)
my.corpus <- tm_map(my.corpus, removePunctuation)
my.corpus <- tm_map(my.corpus, removeNumbers)
my.corpus <- tm_map(my.corpus, content_transformer(tolower))
my.corpus <- tm_map(my.corpus, stemDocument, language = "english")
my.corpus <- tm_map(my.corpus, removeWords, stopwords("english"))
tdm <- TermDocumentMatrix(my.corpus)
inspect(tdm[1:100, 1:3])
c(apply(tdm,1,sum))
