term_vec[[i]] <- GetTermVector(DirSource(catego[i]))
}
# library(SnowballC)
library(tm)
GetTermVector <- function(file_dir){
my.corpus <- Corpus(file_dir, readerControl = list(language = "lat"))
my.corpus <- tm_map(my.corpus, function(x) gsub("[^[:alnum:]]", " ", x))
my.corpus <- tm_map(my.corpus, stripWhitespace)
my.corpus <- tm_map(my.corpus, removePunctuation)
my.corpus <- tm_map(my.corpus, removeNumbers)
my.corpus <- tm_map(my.corpus, tolower)
# for some version need transformer : tm_map(my.corpus, content_transformer(tolower))
my.corpus <- tm_map(my.corpus, stemDocument, language = "english")
my.corpus <- tm_map(my.corpus, removeWords, stopwords("english"))
my.corpus <- tm_map(my.corpus, PlainTextDocument)
# for some version need reset : Corpus(VectorSource(my.corpus))
return(data.frame(as.list(apply(TermDocumentMatrix(my.corpus), 1, sum))))
}
ShortVectoLong <- function(long_term, short_vec){
if(length(short_vec)==0) return(long_vec <- rep(1, length=length(long_term)))
loc <- which(long_term %in% colnames(short_vec))
loc_word <- long_term[loc]
for(i in 1:length(loc)){
long_vec[loc[i]] <- as.numeric( short_vec[which(loc_word[i] == colnames(short_vec))] )
}
return(long_vec)
}
EM_Algorithm <- function(word_freq, tau, tdm){
TPWM <- as.data.frame(matrix(NA, nrow = nrow(tdm), ncol = ncol(tdm)))
for(i in 1:8){
denom <- apply(tdm,1, function(x)sum(tau*x))
for(j in 1:ncol(tdm)){
TPWM[,j] <- tau[j]*tdm[,j] / denom
}
tau <- apply(TPWM, 2, function(x)sum(word_freq*x) / sum(word_freq))
# log_like <- sum(mapply( function(x,y) y*log(sum(tau*x)),
#                         as.data.frame(t(tdm)),
#                         as.list(word_freq)))
# print(log_like)
}
catego[ which(tau == max(tau)) ]
}
##### main #####
### set all file path
setwd("~/Desktop/text-mining-in-EM-algorithm/test")
catego <- list.dirs('Train', recursive=FALSE)
### generate term freq. in each topic
term_vec <- list()
for(i in 1: length(catego)){
term_vec[[i]] <- GetTermVector(DirSource(catego[i]))
}
### All term for EM analysis
all_term <- colnames(term_vec[[1]])
for(i in 2:length(catego)){
all_term <- union(all_term, colnames(term_vec[[i]]))
}
### generate term document matrix
tdm <- as.data.frame(matrix(NA, nrow = length(all_term), ncol = length(catego)))
for(i in 1:length(catego)){
long_vec <- ShortVectoLong(all_term, term_vec[[i]])
tdm[,i] <- long_vec / sum(long_vec)
}
# View(tdm)
### handle query document
all_test <- DirSource('Test')
all_test <- lapply(all_test$filelist, function(x)substring(x,6,9))
all_test <- paste0(rep('Test/',length(all_test)),
as.character(sort(as.numeric(all_test))))
all_test <- all_test
### EM Algorithm
ans_list <- vector(mode="character",length=length(all_test))
names(ans_list) <- 1:length(all_test)
for(i in 1:length(all_test)){
query_term <- GetTermVector(VectorSource(readLines(all_test[i])))
ans_list[i] <- EM_Algorithm(word_freq = ShortVectoLong(all_term, query_term),
tau = rep(1/length(catego), length(catego)),
tdm = tdm
)
print(paste(i,ans_list[i],collapse=" "))
}
query_term <- GetTermVector(VectorSource(readLines(all_test[i])))
ans_list[i] <- EM_Algorithm(word_freq = ShortVectoLong(all_term, query_term),
tau = rep(1/length(catego), length(catego)),
tdm = tdm
)
i
all_term
query_term
word_freq = ShortVectoLong(all_term, query_term)
rep(1,length=length(long_term))
rep(1,length=length(long_term))
print( paste(i, ans_list[i], collapse = " "))
library(SnowballC)
library(tm)
GetTermVector <- function(file_dir){
my.corpus <- Corpus(file_dir, readerControl = list(language = "lat"))
my.corpus <- tm_map(my.corpus, function(x) gsub("[^[:alnum:]]", " ", x))
my.corpus <- tm_map(my.corpus, stripWhitespace)
my.corpus <- tm_map(my.corpus, removePunctuation)
my.corpus <- tm_map(my.corpus, removeNumbers)
my.corpus <- tm_map(my.corpus, tolower)
# for some version need transformer : tm_map(my.corpus, content_transformer(tolower))
my.corpus <- tm_map(my.corpus, stemDocument, language = "english")
my.corpus <- tm_map(my.corpus, removeWords, stopwords("english"))
my.corpus <- tm_map(my.corpus, PlainTextDocument)
# for some version need reset : Corpus(VectorSource(my.corpus))
return(data.frame(as.list(apply(TermDocumentMatrix(my.corpus), 1, sum))))
}
ShortVectoLong <- function(long_term, short_vec){
if(length(short_vec)==0) return(rep(1,length=length(long_term)))
loc <- which(long_term %in% colnames(short_vec))
loc_word <- long_term[loc]
long_vec <- vector(mode="integer",length=length(long_term))
for(i in 1:length(loc)){
long_vec[loc[i]] <- as.numeric( short_vec[which(loc_word[i] == colnames(short_vec))] )
}
return(long_vec)
}
EM_Algorithm <- function(word_freq, tau, tdm){
TPWM <- as.data.frame(matrix(NA, nrow = nrow(tdm), ncol = ncol(tdm)))
for(i in 1:8){
denom <- apply(tdm,1, function(x)sum(tau*x))
for(j in 1:ncol(tdm)){
TPWM[,j] <- tau[j]*tdm[,j] / denom
}
tau <- apply(TPWM, 2, function(x)sum(word_freq*x) / sum(word_freq))
# log_like <- sum(mapply( function(x,y) y*log(sum(tau*x)),
#                         as.data.frame(t(tdm)),
#                         as.list(word_freq)))
# print(log_like)
}
catego[ which(tau == max(tau)) ]
}
##### main #####
### set all file path
setwd("~/Desktop/text-mining-in-EM-algorithm/test")
catego <- list.dirs('Train', recursive=FALSE)
### generate term freq. in each topic
term_vec <- list()
for(i in 1: length(catego)){
term_vec[[i]] <- GetTermVector(DirSource(catego[i]))
}
### All term for EM analysis
all_term <- colnames(term_vec[[1]])
for(i in 2:length(catego)){
all_term <- union(all_term, colnames(term_vec[[i]]))
}
### generate term document matrix
tdm <- as.data.frame(matrix(NA, nrow = length(all_term), ncol = length(catego)))
for(i in 1:length(catego)){
long_vec <- ShortVectoLong(all_term, term_vec[[i]])
tdm[,i] <- long_vec / sum(long_vec)
}
# View(tdm)
### handle query document
all_test <- DirSource('Test')
all_test <- lapply(all_test$filelist, function(x)substring(x,6,9))
all_test <- paste0(rep('Test/',length(all_test)),
as.character(sort(as.numeric(all_test))))
all_test <- all_test
### EM Algorithm
ans_list <- vector(mode="character",length=length(all_test))
names(ans_list) <- 1:length(all_test)
for(i in 1:length(all_test)){
query_term <- GetTermVector(VectorSource(readLines(all_test[i])))
ans_list[i] <- EM_Algorithm(word_freq = ShortVectoLong(all_term, query_term),
tau = rep(1/length(catego), length(catego)),
tdm = tdm
)
print( paste(i, ans_list[i], collapse = " "))
}
all_test[353]
readLines(all_test[353])
readLines(all_test[353])
i = 353
query_term <- GetTermVector(VectorSource(readLines(all_test[i])))
query_term
ans_list[i] <- EM_Algorithm(word_freq = ShortVectoLong(all_term, query_term),
tau = rep(1/length(catego), length(catego)),
tdm = tdm
)
ShortVectoLong(all_term, query_term)
all_test[353]
ShortVectoLong(all_term, query_term)
sum(ShortVectoLong(all_term, query_term))
ShortVectoLong(all_term, query_term)
query_term
ans_list[i] <- EM_Algorithm(word_freq = ShortVectoLong(all_term, query_term),
tau = rep(1/length(catego), length(catego)),
tdm = tdm
)
word_freq = ShortVectoLong(all_term, query_term),
word_freq = ShortVectoLong(all_term, query_term)
tau = rep(1/length(catego), length(catego))
tdm = tdm
word_freq
tau
TPWM <- as.data.frame(matrix(NA, nrow = nrow(tdm), ncol = ncol(tdm)))
denom <- apply(tdm,1, function(x)sum(tau*x))
for(j in 1:ncol(tdm)){
TPWM[,j] <- tau[j]*tdm[,j] / denom
}
tau <- apply(TPWM, 2, function(x)sum(word_freq*x) / sum(word_freq))
catego[ which(tau == max(tau)) ]
for(i in 1:8){
denom <- apply(tdm,1, function(x)sum(tau*x))
for(j in 1:ncol(tdm)){
TPWM[,j] <- tau[j]*tdm[,j] / denom
}
tau <- apply(TPWM, 2, function(x)sum(word_freq*x) / sum(word_freq))
# log_like <- sum(mapply( function(x,y) y*log(sum(tau*x)),
#                         as.data.frame(t(tdm)),
#                         as.list(word_freq)))
# print(log_like)
}
catego[ which(tau == max(tau)) ]
tau
word_freq
sum(word_freq)
tau
sum(word_freq*x)
apply(TPWM, 2, function(x)sum(word_freq*x) / sum(word_freq))
TPWM
ShortVectoLong(all_term, query_term)
readLines(all_test[i]))
readLines(all_test[i])
readLines(all_test[353])
i = 353
query_term <- GetTermVector(VectorSource(readLines(all_test[i])))
word_freq = ShortVectoLong(all_term, query_term)
tau = rep(1/length(catego), length(catego))
tdm = tdm
query_term <- GetTermVector(VectorSource(readLines(all_test[353])))
word_freq = ShortVectoLong(all_term, query_term)
tau = rep(1/length(catego), length(catego))
tdm = tdm
TPWM <- as.data.frame(matrix(NA, nrow = nrow(tdm), ncol = ncol(tdm)))
for(i in 1:2){
denom <- apply(tdm,1, function(x)sum(tau*x))
for(j in 1:ncol(tdm)){
TPWM[,j] <- tau[j]*tdm[,j] / denom
}
tau <- apply(TPWM, 2, function(x)sum(word_freq*x) / sum(word_freq))
# log_like <- sum(mapply( function(x,y) y*log(sum(tau*x)),
#                         as.data.frame(t(tdm)),
#                         as.list(word_freq)))
# print(log_like)
}
catego[ which(tau == max(tau)) ]
# test
query_term <- GetTermVector(VectorSource(readLines(all_test[353])))
word_freq = ShortVectoLong(all_term, query_term)
tau = rep(1/length(catego), length(catego))
tdm = tdm
TPWM <- as.data.frame(matrix(NA, nrow = nrow(tdm), ncol = ncol(tdm)))
denom <- apply(tdm,1, function(x)sum(tau*x))
denom
for(j in 1:ncol(tdm)){
TPWM[,j] <- tau[j]*tdm[,j] / denom
}
TPWM
tau <- apply(TPWM, 2, function(x)sum(word_freq*x) / sum(word_freq))
tau
catego[ which(tau == max(tau)) ]
denom <- apply(tdm,1, function(x)sum(tau*x))
for(j in 1:ncol(tdm)){
TPWM[,j] <- tau[j]*tdm[,j] / denom
}
tau <- apply(TPWM, 2, function(x)sum(word_freq*x) / sum(word_freq))
# log_like <- sum(mapply( function(x,y) y*log(sum(tau*x)),
#                         as.data.frame(t(tdm)),
#                         as.list(word_freq)))
# print(log_like)
catego[ which(tau == max(tau)) ]
tau
denom
TPWM
sum(word_freq)
TPWM
sum(word_freq*[TPWM[,1]])
TPWM[,1]
sum(word_freq*TPWM[,1])
word_freq*TPWM[,1]
sum(word_freq*TPWM[,1])
word_freq
sum(word_freq)
sum(TPWM[,1])
sum(TPWM[,2])
sum(as.numeric(TPWM[,2]))
TPWM[,2]
TPWM[which(TPWM!=0),2]
TPWM[,2]
sum(TPWM[,2])
sum(TPWM[1:10,2])
sum(TPWM[1:100,2])
sum(TPWM[1:1000,2])
sum(TPWM[1:10000,2])
sum(TPWM[1:100000,2])
sum(TPWM[1:10000,2])
sum(TPWM[1:nrow(TPWM),2])
sum(TPWM[1:nrow(TPWM),2])
nrow(TPWM)
sum(TPWM[1:30000,2])
sum(TPWM[20000:30000,2])
TPWM[20000:30000,2]
sum(TPWM[20000:30000,2])
sum(TPWM[30000:30000,2])
sum(TPWM[31000:30000,2])
sum(TPWM[2900:30000,2])
sum(TPWM[29000:30000,2])
sum(TPWM[28000:30000,2])
sum(TPWM[27000:30000,2])
sum(TPWM[23000:30000,2])
sum(TPWM[20000:30000,2])
sum(TPWM[21000:30000,2])
sum(TPWM[20500:30000,2])
sum(TPWM[20000:21000,2])
sum(TPWM[20000:20500,2])
sum(TPWM[20000:20250,2])
sum(TPWM[20000:20125,2])
sum(TPWM[20000:20100,2])
sum(TPWM[20000:2050,2])
sum(TPWM[20000:20500,2])
sum(TPWM[20000:20075,2])
sum(TPWM[20000:20030,2])
sum(TPWM[20000:20015,2])
sum(TPWM[20000:20006,2])
sum(TPWM[20000:20005,2])
sum(TPWM[20000:20004,2])
sum(TPWM[20000:20003,2])
sum(TPWM[20000:20002,2])
sum(TPWM[20000:20001,2])
sum(TPWM[20000:20002,2])
sum(TPWM[20002,2])
sum(TPWM[20001,2])
sum(TPWM[20003,2])
sum(TPWM[20004,2])
sum(TPWM[20005,2])
sum(TPWM[20007,2])
sum(TPWM[20006,2])
sum(TPWM[20005,2])
sum(TPWM[200010,2])
sum(TPWM[200011,2])
sum(TPWM[200030,2])
TPWM[200030,2]
TPWM[20030,2]
TPWM[20000,2]
TPWM[20003,2]
tau
sum(word_freq))
sum(word_freq)
TPWM
length(TPWM[which(TPWM==NaN)]
)
length(TPWM[which(is.NaN)]
)
length(TPWM[which(is.nan(TPWM))]
)
length(TPWM[which(is.nan(TPWM[,]))])
length(TPWM[which(is.nan(TPWM[1,]))])
length(TPWM[which(is.nan(TPWM[,1]))])
apply(TPWM, 1, is.nan)
apply(TPWM, c(1,2), is.nan)
sum(apply(TPWM, c(1,2), is.nan))
apply(TPWM, c(1,2), which(is.nan))
apply(TPWM, c(1,2), function(x)which(is.nan(x)))
apply(TPWM, c(1,2), is.nan(x))
apply(TPWM, c(1,2), is.nan)
sum(apply(TPWM, c(1,2), is.nan))
library(SnowballC)
library(tm)
GetTermVector <- function(file_dir){
my.corpus <- Corpus(file_dir, readerControl = list(language = "lat"))
my.corpus <- tm_map(my.corpus, function(x) gsub("[^[:alnum:]]", " ", x))
my.corpus <- tm_map(my.corpus, stripWhitespace)
my.corpus <- tm_map(my.corpus, removePunctuation)
my.corpus <- tm_map(my.corpus, removeNumbers)
my.corpus <- tm_map(my.corpus, tolower)
# for some version need transformer : tm_map(my.corpus, content_transformer(tolower))
my.corpus <- tm_map(my.corpus, stemDocument, language = "english")
my.corpus <- tm_map(my.corpus, removeWords, stopwords("english"))
my.corpus <- tm_map(my.corpus, PlainTextDocument)
# for some version need reset : Corpus(VectorSource(my.corpus))
return(data.frame(as.list(apply(TermDocumentMatrix(my.corpus), 1, sum))))
}
ShortVectoLong <- function(long_term, short_vec){
if(length(short_vec)==0) return(rep(1,length=length(long_term)))
loc <- which(long_term %in% colnames(short_vec))
loc_word <- long_term[loc]
long_vec <- vector(mode="integer",length=length(long_term))
for(i in 1:length(loc)){
long_vec[loc[i]] <- as.numeric( short_vec[which(loc_word[i] == colnames(short_vec))] )
}
return(long_vec)
}
EM_Algorithm <- function(word_freq, tau, tdm){
TPWM <- as.data.frame(matrix(NA, nrow = nrow(tdm), ncol = ncol(tdm)))
for(i in 1:8){
denom <- apply(tdm,1, function(x)sum(tau*x))
for(j in 1:ncol(tdm)){
TPWM[,j] <- tau[j]*tdm[,j] / denom
}
tau <- apply(TPWM, 2, function(x)sum(word_freq*x) / sum(word_freq))
# log_like <- sum(mapply( function(x,y) y*log(sum(tau*x)),
#                         as.data.frame(t(tdm)),
#                         as.list(word_freq)))
# print(log_like)
}
catego[ which(tau == max(tau)) ]
}
##### main #####
### set all file path
setwd("~/Desktop/text-mining-in-EM-algorithm/test")
catego <- list.dirs('Train', recursive=FALSE)
### generate term freq. in each topic
term_vec <- list()
for(i in 1: length(catego)){
term_vec[[i]] <- GetTermVector(DirSource(catego[i]))
}
### All term for EM analysis
all_term <- colnames(term_vec[[1]])
for(i in 2:length(catego)){
all_term <- union(all_term, colnames(term_vec[[i]]))
}
### generate term document matrix
tdm <- as.data.frame(matrix(NA, nrow = length(all_term), ncol = length(catego)))
for(i in 1:length(catego)){
long_vec <- ShortVectoLong(all_term, term_vec[[i]])
tdm[,i] <- long_vec / sum(long_vec)
}
# View(tdm)
### handle query document
all_test <- DirSource('Test')
all_test <- lapply(all_test$filelist, function(x)substring(x,6,9))
all_test <- paste0(rep('Test/',length(all_test)),
as.character(sort(as.numeric(all_test))))
all_test <- all_test
### EM Algorithm
ans_list <- vector(mode="character",length=length(all_test))
names(ans_list) <- 1:length(all_test)
for(i in 1:length(all_test)){
query_term <- GetTermVector(VectorSource(readLines(all_test[i])))
ans_list[i] <- EM_Algorithm(word_freq = ShortVectoLong(all_term, query_term),
tau = rep(1/length(catego), length(catego)),
tdm = tdm
)
print( paste(i, ans_list[i], collapse = " "))
}
for(i in 354:length(all_test)){
query_term <- GetTermVector(VectorSource(readLines(all_test[i])))
ans_list[i] <- EM_Algorithm(word_freq = ShortVectoLong(all_term, query_term),
tau = rep(1/length(catego), length(catego)),
tdm = tdm
)
print( paste(i, ans_list[i], collapse = " "))
}
readLines(all_test[1136]))
readLines(all_test[1136])
GetTermVector(VectorSource(readLines(all_test[1136])))
for(i in 1137:length(all_test)){
query_term <- GetTermVector(VectorSource(readLines(all_test[i])))
ans_list[i] <- EM_Algorithm(word_freq = ShortVectoLong(all_term, query_term),
tau = rep(1/length(catego), length(catego)),
tdm = tdm
)
print( paste(i, ans_list[i], collapse = " "))
}
readLines(all_test[1794]))
readLines(all_test[1794])
for(i in 1795:length(all_test)){
query_term <- GetTermVector(VectorSource(readLines(all_test[i])))
ans_list[i] <- EM_Algorithm(word_freq = ShortVectoLong(all_term, query_term),
tau = rep(1/length(catego), length(catego)),
tdm = tdm
)
print( paste(i, ans_list[i], collapse = " "))
}
readLines(all_test[1884])
for(i in 1885:length(all_test)){
query_term <- GetTermVector(VectorSource(readLines(all_test[i])))
ans_list[i] <- EM_Algorithm(word_freq = ShortVectoLong(all_term, query_term),
tau = rep(1/length(catego), length(catego)),
tdm = tdm
)
print( paste(i, ans_list[i], collapse = " "))
}
readLines(all_test[2055])
for(i in 2056:length(all_test)){
query_term <- GetTermVector(VectorSource(readLines(all_test[i])))
ans_list[i] <- EM_Algorithm(word_freq = ShortVectoLong(all_term, query_term),
tau = rep(1/length(catego), length(catego)),
tdm = tdm
)
print( paste(i, ans_list[i], collapse = " "))
}
for(i in 2200:length(all_test)){
query_term <- GetTermVector(VectorSource(readLines(all_test[i])))
ans_list[i] <- EM_Algorithm(word_freq = ShortVectoLong(all_term, query_term),
tau = rep(1/length(catego), length(catego)),
tdm = tdm
)
print( paste(i, ans_list[i], collapse = " "))
}
for(i in 2624:length(all_test)){
query_term <- GetTermVector(VectorSource(readLines(all_test[i])))
ans_list[i] <- EM_Algorithm(word_freq = ShortVectoLong(all_term, query_term),
tau = rep(1/length(catego), length(catego)),
tdm = tdm
)
print( paste(i, ans_list[i], collapse = " "))
}
save.image("~/Desktop/temp.RData")
